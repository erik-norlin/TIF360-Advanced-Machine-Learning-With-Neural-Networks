{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install deeptrack\n",
        "!pip install ipython-autotime\n",
        "!pip install sklearn\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "id": "XpfN4LU0ag6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import deeptrack as dt\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Reshape, UpSampling2D, Input, BatchNormalization, LayerNormalization, Activation, Layer, InputLayer, concatenate, Add, Dropout, LSTM"
      ],
      "metadata": {
        "id": "OM01aC6tlFz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **a)**"
      ],
      "metadata": {
        "id": "6XsgtNrjcA5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "particle = dt.Sphere(\n",
        "    intensity=lambda: 10 + 10 * np.random.rand(),\n",
        "    radius=lambda: MIN_SIZE + np.random.rand() * (MAX_SIZE - MIN_SIZE),\n",
        "    position=lambda: IMAGE_SIZE * np.random.rand(2),\n",
        "    vel=lambda: MAX_VEL * np.random.rand(2),\n",
        "    position_unit=\"pixel\",\n",
        ")\n",
        "\n",
        "train_data_a = []\n",
        "for i in range(10000):\n",
        "  sample = optics(particle) >> dt.Gaussian(sigma=0.1)\n",
        "  train_data_a.append(sample.update()())\n",
        "\n",
        "val_data_a = []\n",
        "for i in range(2000):\n",
        "  sample = optics(particle) >> dt.Gaussian(sigma=0.1)\n",
        "  val_data_a.append(sample.update()())"
      ],
      "metadata": {
        "id": "Rl9MpMM1W32z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "F13UqicCp6F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.save(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/3a/train_data_a.npy\", train_data_a)\n",
        "# np.save(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/3a/val_data_a.npy\", val_data_a)"
      ],
      "metadata": {
        "id": "IwxAPB8VbnnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_a = np.load(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/3a/train_data_a.npy\")\n",
        "val_data_a = np.load(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/3a/val_data_a.npy\")\n",
        "\n",
        "for i in range(10000):\n",
        "  train_data_a[i] = train_data_a[i] / np.max(train_data_a[i])\n",
        "\n",
        "for i in range(2000):\n",
        "  val_data_a[i] = val_data_a[i] / np.max(val_data_a[i])"
      ],
      "metadata": {
        "id": "rqClJc_CU5Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bottle_nodes = 10\n",
        "\n",
        "encoder = tf.keras.models.Sequential()\n",
        "encoder.add(Conv2D(32, (3,3), (1,1), activation=\"relu\", padding=\"same\", input_shape=(64,64,1)))\n",
        "encoder.add(MaxPooling2D((2,2), (2,2)))\n",
        "\n",
        "encoder.add(Conv2D(64, (3,3), (1,1), activation=\"relu\", padding=\"same\"))\n",
        "encoder.add(MaxPooling2D((2,2), (2,2)))\n",
        "\n",
        "encoder.add(Conv2D(128, (3,3), (1,1), activation=\"relu\", padding=\"same\"))\n",
        "encoder.add(MaxPooling2D((2,2), (2,2)))\n",
        "\n",
        "encoder.add(Flatten())\n",
        "encoder.add(Dense(128, activation=\"relu\"))\n",
        "encoder.add(Dense(bottle_nodes))\n",
        "\n",
        "\n",
        "\n",
        "decoder = tf.keras.models.Sequential()\n",
        "decoder.add(Dense(128, activation=\"relu\"))\n",
        "decoder.add(Dense(8*8*128, activation=\"relu\"))\n",
        "decoder.add(Reshape((8,8,128)))\n",
        "\n",
        "decoder.add(UpSampling2D((2,2)))\n",
        "decoder.add(Conv2D(128, (3,3), (1,1), activation=\"relu\", padding=\"same\"))\n",
        "\n",
        "decoder.add(UpSampling2D((2,2)))\n",
        "decoder.add(Conv2D(64, (3,3), (1,1), activation=\"relu\", padding=\"same\"))\n",
        "\n",
        "decoder.add(UpSampling2D((2,2)))\n",
        "decoder.add(Conv2D(32, (3,3), (1,1), activation=\"relu\", padding=\"same\"))\n",
        "decoder.add(Conv2D(1, (3,3), (1,1), padding=\"same\")) #change to linear\n",
        "\n",
        "input=Input((64,64,1))\n",
        "autoencoder = tf.keras.models.Model(inputs=input, outputs=decoder(encoder(input)))\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "autoencoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "autoencoder.summary()\n",
        "# (I+P-K+S)/S"
      ],
      "metadata": {
        "id": "jBmArI7b7KGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = np.array(train_data_a[0])\n",
        "plt.figure()\n",
        "plt.imshow(test)\n",
        "\n",
        "test = tf.expand_dims(test, axis=0)\n",
        "test_pred = autoencoder.predict(test, verbose=0)\n",
        "plt.figure()\n",
        "plt.imshow(test_pred[0])"
      ],
      "metadata": {
        "id": "3BffESz4LtJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)\n",
        "history = autoencoder.fit(x=np.array(train_data_a), y=np.array(train_data_a), validation_data=(np.array(val_data_a), np.array(val_data_a)), callbacks = [callback], epochs=20, verbose=1)"
      ],
      "metadata": {
        "id": "SmwQ0DXnG763"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder.save(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/encoder.h5\") # min loss 1*e-4,  latent space 4\n",
        "# decoder.save(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/decoder.h5\")\n",
        "# autoencoder.save(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/autoencoder.h5\")"
      ],
      "metadata": {
        "id": "rfQUgnWfUbGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder.save(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/encoder_b.h5\") # min loss 6*e-5,  latent space 10\n",
        "# decoder.save(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/decoder_b.h5\")\n",
        "# autoencoder.save(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/autoencoder_b.h5\")"
      ],
      "metadata": {
        "id": "pWWlVpKifCzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# epochs = history.epoch\n",
        "# train_loss = history.history['loss']\n",
        "# val_loss = history.history['val_loss']\n",
        "\n",
        "# plt.figure()\n",
        "# plt.plot(epochs, train_loss, label=\"Training loss\")\n",
        "# plt.plot(epochs, val_loss, label=\"Validation loss\")\n",
        "# plt.legend(loc=\"upper right\", fontsize=10)\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.xlim([epochs[0], epochs[-1]])\n",
        "# title = \"loss.png\".format(bottle_nodes)\n",
        "# # plt.savefig(location+title)\n",
        "\n",
        "test = np.array(train_data_a[149])#14, 100, 11\n",
        "plt.figure()\n",
        "plt.imshow(test)\n",
        "title = \"3a_n={}_no.1_TRUE.png\".format(bottle_nodes)\n",
        "# plt.savefig(location+title)\n",
        "\n",
        "test = tf.expand_dims(test, axis=0)\n",
        "test_pred = autoencoder.predict(test)\n",
        "plt.figure()\n",
        "plt.imshow(test_pred[0])\n",
        "title = \"3a_n={}_no.1__PRED.png\".format(bottle_nodes)\n",
        "# plt.savefig(location+title)\n",
        "\n",
        "for seq in dataset_b:\n",
        "  seq_b = seq[0]\n",
        "  break\n",
        "\n",
        "test = tf.expand_dims(seq_b, axis=0)\n",
        "test = test / np.max(test)\n",
        "test_pred = autoencoder.predict(test)\n",
        "plt.figure()\n",
        "plt.imshow(test_pred[0])\n",
        "title = \"3a_n={}_no.1__PRED.png\".format(bottle_nodes)\n",
        "# plt.savefig(location+title)"
      ],
      "metadata": {
        "id": "rVMuOLXTTIPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **b)**"
      ],
      "metadata": {
        "id": "odrAfQVnb2pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bottle_nodes = 10\n",
        "encoder = tf.keras.models.load_model(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/encoder_b.h5\")\n",
        "decoder = tf.keras.models.load_model(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/decoder_b.h5\")\n",
        "autoencoder = tf.keras.models.load_model(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/autoencoder_b.h5\")"
      ],
      "metadata": {
        "id": "hPLTktTYWbMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjQ3cY7Nac0l"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 64\n",
        "sequence_length = 10  # Number of frames per sequence\n",
        "MIN_SIZE = 0.5e-6\n",
        "MAX_SIZE = 1.5e-6\n",
        "MAX_VEL = 10  # Maximum velocity. The higher the trickier!\n",
        "MAX_PARTICLES = 3  # Max number of particles in each sequence. The higher the trickier!\n",
        "\n",
        "# Defining properties of the particles\n",
        "particle = dt.Sphere(\n",
        "    intensity=lambda: 10 + 10 * np.random.rand(),\n",
        "    radius=lambda: MIN_SIZE + np.random.rand() * (MAX_SIZE - MIN_SIZE),\n",
        "    position=lambda: IMAGE_SIZE * np.random.rand(2),\n",
        "    vel=lambda: MAX_VEL * np.random.rand(2),\n",
        "    position_unit=\"pixel\",\n",
        ")\n",
        "\n",
        "# Defining an update rule for the particle position\n",
        "def get_position(previous_value, vel):\n",
        "\n",
        "    newv = previous_value + vel\n",
        "    for i in range(2):\n",
        "        if newv[i] > 63:\n",
        "            newv[i] = 63 - np.abs(newv[i] - 63)\n",
        "            vel[i] = -vel[i]\n",
        "        elif newv[i] < 0:\n",
        "            newv[i] = np.abs(newv[i])\n",
        "            vel[i] = -vel[i]\n",
        "    return newv\n",
        "\n",
        "\n",
        "particle = dt.Sequential(particle, position=get_position)\n",
        "\n",
        "# Defining properties of the microscope\n",
        "optics = dt.Fluorescence(\n",
        "    NA=1,\n",
        "    output_region=(0, 0, IMAGE_SIZE, IMAGE_SIZE),\n",
        "    magnification=10,\n",
        "    resolution=(1e-6, 1e-6, 1e-6),\n",
        "    wavelength=633e-9,\n",
        ")\n",
        "\n",
        "# Combining everything into a dataset.\n",
        "# Note that the sequences are flipped in different directions, so that each unique sequence defines\n",
        "# in fact 8 sequences flipped in different directions, to speed up data generation\n",
        "sequential_images = dt.Sequence(\n",
        "    optics(particle ** (lambda: 1 + np.random.randint(MAX_PARTICLES))),\n",
        "    sequence_length=sequence_length,\n",
        ")\n",
        "dataset_b = sequential_images >> dt.FlipUD() >> dt.FlipDiagonal() >> dt.FlipLR()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_out_list = []\n",
        "encoder_label_list = []\n",
        "\n",
        "c = 0\n",
        "for seq in dataset_b:\n",
        "\n",
        "  train_seq = tf.convert_to_tensor(seq[:sequence_length-1]).numpy()\n",
        "  for i in range(train_seq.shape[0]):\n",
        "    train_seq[i] = train_seq[i] / np.max(train_seq[i])\n",
        "\n",
        "  label = tf.expand_dims(np.array(seq[sequence_length-1]), 0).numpy()\n",
        "  label = label / np.max(label)\n",
        "\n",
        "  encoder_out = encoder.predict(train_seq, verbose=0)\n",
        "  encoder_label = encoder.predict(label, verbose=0)[0]\n",
        "\n",
        "  encoder_out_list.append(encoder_out)\n",
        "  encoder_label_list.append(encoder_label)\n",
        "\n",
        "  c += 1\n",
        "  if c % 10 == 0:\n",
        "    print(c, encoder_out.shape)\n",
        "\n",
        "  if c % 100 == 0:\n",
        "    encoder_outs = np.stack(encoder_out_list)\n",
        "    encoder_labels = np.stack(encoder_label_list)\n",
        "    print(\"Inputs: \", encoder_outs.shape, \"Labels: \", encoder_labels.shape)\n",
        "\n",
        "    # np.save(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/3b/encoder_outs_10.npy\", encoder_outs)\n",
        "    # np.save(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/3b/encoder_labels_10.npy\", encoder_labels)"
      ],
      "metadata": {
        "id": "x7lYFLGYm6WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder_outs = np.load(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/3b/encoder_outs.npy\")\n",
        "# encoder_labels = np.load(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/3b/encoder_labels.npy\")\n",
        "# print(encoder_outs.shape, encoder_labels.shape)\n",
        "\n",
        "encoder_outs = np.load(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/3b/encoder_outs_10.npy\")\n",
        "encoder_labels = np.load(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/3b/encoder_labels_10.npy\")\n",
        "print(encoder_outs.shape, encoder_labels.shape)\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(encoder_outs, encoder_labels, test_size=0.2, shuffle= True)\n",
        "print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)"
      ],
      "metadata": {
        "id": "EVzpzv_9cSEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Time2Vector(Layer):\n",
        "  def __init__(self, seq_len, **kwargs):\n",
        "    super(Time2Vector, self).__init__()\n",
        "    self.seq_len = seq_len\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.weights_linear = self.add_weight(name='weight_linear', shape=(int(self.seq_len),), initializer='uniform', trainable=True)\n",
        "    self.bias_linear = self.add_weight(name='bias_linear', shape=(int(self.seq_len),), initializer='uniform', trainable=True)\n",
        "    self.weights_periodic = self.add_weight(name='weight_periodic', shape=(int(self.seq_len),), initializer='uniform', trainable=True)\n",
        "    self.bias_periodic = self.add_weight(name='bias_periodic', shape=(int(self.seq_len),), initializer='uniform', trainable=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = tf.math.reduce_mean(x[:,:,:], axis=-1)\n",
        "    time_linear = self.weights_linear * x + self.bias_linear\n",
        "    time_linear = tf.expand_dims(time_linear, axis=-1)\n",
        "    time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
        "    time_periodic = tf.expand_dims(time_periodic, axis=-1)\n",
        "    return tf.concat([time_linear, time_periodic], axis=-1)\n",
        "\n",
        "\n",
        "class SingleAttention(Layer):\n",
        "  def __init__(self, d_k, d_v):\n",
        "    super(SingleAttention, self).__init__()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.query = Dense(self.d_k, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
        "    self.key = Dense(self.d_k, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
        "    self.value = Dense(self.d_v, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
        "    q = self.query(inputs[0])\n",
        "    k = self.key(inputs[1])\n",
        "\n",
        "    attn_weights = tf.matmul(q, k, transpose_b=True)\n",
        "    attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
        "    attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
        "\n",
        "    v = self.value(inputs[2])\n",
        "    attn_out = tf.matmul(attn_weights, v)\n",
        "    return attn_out"
      ],
      "metadata": {
        "id": "IMA5nCKehjh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze weights of autoencoder\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "for k,v in autoencoder._get_trainable_state().items():\n",
        "    k.trainable = False\n",
        "autoencoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "for k,v in encoder._get_trainable_state().items():\n",
        "    k.trainable = False\n",
        "encoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "for k,v in decoder._get_trainable_state().items():\n",
        "    k.trainable = False\n",
        "decoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "\n",
        "\n",
        "# Transformer encoder\n",
        "input = Input(shape=(sequence_length-1, bottle_nodes,))\n",
        "\n",
        "time_vec = Time2Vector(sequence_length-1)(input)\n",
        "embedding = concatenate([input, time_vec])\n",
        "embedding_input = (embedding, embedding, embedding)\n",
        "d = embedding.shape[-1]\n",
        "\n",
        "attention1 = SingleAttention(d, d)(embedding_input)\n",
        "attention2 = SingleAttention(d, d)(embedding_input)\n",
        "attention3 = SingleAttention(d, d)(embedding_input)\n",
        "\n",
        "attention_concat = concatenate([attention1, attention2, attention3])\n",
        "multihead = Dense(embedding.shape[-1], activation='relu')(attention_concat)\n",
        "\n",
        "\n",
        "dropout = Dropout(0.5)(multihead)\n",
        "add_norm = LayerNormalization()(Add()([embedding, dropout]))\n",
        "\n",
        "linear = Dense(d, activation='relu')(add_norm)\n",
        "linear = Dense(d)(linear)\n",
        "\n",
        "dropout = Dropout(0.5)(linear)\n",
        "add_norm = LayerNormalization()(Add()([embedding, dropout]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "embedding_input = (add_norm, add_norm, add_norm)\n",
        "attention1 = SingleAttention(d, d)(embedding_input)\n",
        "attention2 = SingleAttention(d, d)(embedding_input)\n",
        "attention3 = SingleAttention(d, d)(embedding_input)\n",
        "\n",
        "attention_concat = concatenate([attention1, attention2, attention3])\n",
        "multihead = Dense(embedding.shape[-1], activation='relu')(attention_concat)\n",
        "\n",
        "\n",
        "dropout = Dropout(0.5)(multihead)\n",
        "add_norm = LayerNormalization()(Add()([embedding, dropout]))\n",
        "\n",
        "linear = Dense(d, activation='relu')(add_norm)\n",
        "linear = Dense(d)(linear)\n",
        "\n",
        "dropout = Dropout(0.5)(linear)\n",
        "add_norm = LayerNormalization()(Add()([embedding, dropout]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "embedding_input = (add_norm, add_norm, add_norm)\n",
        "attention1 = SingleAttention(d, d)(embedding_input)\n",
        "attention2 = SingleAttention(d, d)(embedding_input)\n",
        "attention3 = SingleAttention(d, d)(embedding_input)\n",
        "\n",
        "attention_concat = concatenate([attention1, attention2, attention3])\n",
        "multihead = Dense(embedding.shape[-1], activation='relu')(attention_concat)\n",
        "\n",
        "\n",
        "dropout = Dropout(0.5)(multihead)\n",
        "add_norm = LayerNormalization()(Add()([embedding, dropout]))\n",
        "\n",
        "linear = Dense(d, activation='relu')(add_norm)\n",
        "linear = Dense(d)(linear)\n",
        "\n",
        "dropout = Dropout(0.5)(linear)\n",
        "add_norm = LayerNormalization()(Add()([embedding, dropout]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# embedding_input = (add_norm, add_norm, add_norm)\n",
        "# attention1 = SingleAttention(d, d)(embedding_input)\n",
        "# attention2 = SingleAttention(d, d)(embedding_input)\n",
        "# attention3 = SingleAttention(d, d)(embedding_input)\n",
        "\n",
        "# attention_concat = concatenate([attention1, attention2, attention3])\n",
        "# multihead = Dense(embedding.shape[-1], activation='relu')(attention_concat)\n",
        "\n",
        "\n",
        "# dropout = Dropout(0.5)(multihead)\n",
        "# add_norm = LayerNormalization()(Add()([embedding, dropout]))\n",
        "\n",
        "# linear = Dense(d, activation='relu')(add_norm)\n",
        "# linear = Dense(d)(linear)\n",
        "\n",
        "# dropout = Dropout(0.5)(linear)\n",
        "# add_norm = LayerNormalization()(Add()([embedding, dropout]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "linear = Flatten()(add_norm)\n",
        "linear = Dense(64, activation='relu')(linear)\n",
        "linear = Dense(64, activation='relu')(linear)\n",
        "linear = Dense(64, activation='relu')(linear)\n",
        "\n",
        "linear = Dense(bottle_nodes)(linear)\n",
        "\n",
        "\n",
        "transformer = tf.keras.models.Model(input, linear)\n",
        "transformer.compile(loss=\"mse\", optimizer=optimizer)\n",
        "transformer.summary()"
      ],
      "metadata": {
        "id": "f0d5DuT7qURL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)\n",
        "history = transformer.fit(x_train, y_train, epochs=40, verbose=1, validation_data=(x_valid, y_valid), callbacks=[callback])"
      ],
      "metadata": {
        "id": "aFLVpC9OSMOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# epochs = history.epoch\n",
        "num_epochs = np.linspace(1,20,20)\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, train_loss, label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, label=\"Validation loss\")\n",
        "plt.legend(loc=\"upper right\", fontsize=10)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlim([epochs[0], epochs[-1]])\n",
        "# title = \"loss.png\".format(bottle_nodes)\n",
        "# plt.savefig(location+title)"
      ],
      "metadata": {
        "id": "89jlw3v4VQiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.save(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/transformer.h5\")"
      ],
      "metadata": {
        "id": "g2AN0MJHA5EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = tf.keras.models.load_model(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/transformer.h5\")"
      ],
      "metadata": {
        "id": "SmRHvs7hBJV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze weights of autoencoder\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "for k,v in autoencoder._get_trainable_state().items():\n",
        "    k.trainable = False\n",
        "autoencoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "for k,v in encoder._get_trainable_state().items():\n",
        "    k.trainable = False\n",
        "encoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "for k,v in decoder._get_trainable_state().items():\n",
        "    k.trainable = False\n",
        "decoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "\n",
        "# LSTM model\n",
        "input = Input(shape=(sequence_length-1, bottle_nodes,))\n",
        "lstm = LSTM(512, activation='relu', return_sequences=True)(input)\n",
        "lstm = LSTM(512, activation='relu', return_sequences=True)(lstm)\n",
        "lstm = LSTM(512, activation='relu', return_sequences=True)(lstm)\n",
        "lstm = LSTM(512, activation='relu', return_sequences=False)(lstm)\n",
        "\n",
        "dropout = Dropout(0.5)(lstm)\n",
        "\n",
        "\n",
        "\n",
        "linear = Dense(bottle_nodes)(dropout)\n",
        "\n",
        "lstm = tf.keras.models.Model(input, linear)\n",
        "lstm.compile(loss=\"mse\", optimizer=optimizer)\n",
        "lstm.summary()"
      ],
      "metadata": {
        "id": "nFWBDFiahoCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)\n",
        "history = lstm.fit(x_train, y_train, epochs=40, verbose=1, validation_data=(x_valid, y_valid), callbacks=[callback])"
      ],
      "metadata": {
        "id": "w3gELuz2j-i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# epochs = history.epoch\n",
        "num_epochs = np.linspace(1,40,40)\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(num_epochs, train_loss, label=\"Training loss\")\n",
        "plt.plot(num_epochs, val_loss, label=\"Validation loss\")\n",
        "plt.legend(loc=\"upper right\", fontsize=10)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlim([num_epochs[0], num_epochs[-1]])\n",
        "title = \"loss.png\".format(bottle_nodes)\n",
        "# plt.savefig(location+title)"
      ],
      "metadata": {
        "id": "-aPPfQMWwGuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm.save(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/lstm.h5\")"
      ],
      "metadata": {
        "id": "5p3u3TbVmc2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = tf.keras.models.load_model(\"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/lstm.h5\")"
      ],
      "metadata": {
        "id": "88F0Q5mNmgF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_i = 4\n",
        "true_encoded = tf.expand_dims(encoder_labels[seq_i], 0)\n",
        "true_decoded = decoder.predict(true_encoded, verbose=0)\n",
        "\n",
        "# # Transformer\n",
        "# seq_encoded = tf.expand_dims(encoder_outs[seq_i], 0)\n",
        "# pred_encoded = transformer.predict(seq_encoded, verbose=0)\n",
        "# pred_decoded = decoder.predict(pred_encoded, verbose=0)\n",
        "\n",
        "# LSTM\n",
        "seq_encoded = tf.expand_dims(encoder_outs[seq_i], 0)\n",
        "pred_encoded = lstm.predict(seq_encoded, verbose=0)\n",
        "pred_decoded = decoder.predict(pred_encoded, verbose=0)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(true_decoded[0])\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(pred_decoded[0])"
      ],
      "metadata": {
        "id": "MJKY4AaUQpND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 110\n",
        "\n",
        "IMAGE_SIZE = 64\n",
        "sequence_length = 10  # Number of frames per sequence\n",
        "MIN_SIZE = 0.5e-6\n",
        "MAX_SIZE = 1.5e-6\n",
        "MAX_VEL = 10  # Maximum velocity. The higher the trickier!\n",
        "MAX_PARTICLES = 3  # Max number of particles in each sequence. The higher the trickier!\n",
        "\n",
        "# Defining properties of the particles\n",
        "particle = dt.Sphere(\n",
        "    intensity=lambda: 10 + 10 * np.random.rand(),\n",
        "    radius=lambda: MIN_SIZE + np.random.rand() * (MAX_SIZE - MIN_SIZE),\n",
        "    position=lambda: IMAGE_SIZE * np.random.rand(2),\n",
        "    vel=lambda: MAX_VEL * np.random.rand(2),\n",
        "    position_unit=\"pixel\",\n",
        ")\n",
        "\n",
        "# Defining an update rule for the particle position\n",
        "def get_position(previous_value, vel):\n",
        "\n",
        "    newv = previous_value + vel\n",
        "    for i in range(2):\n",
        "        if newv[i] > 63:\n",
        "            newv[i] = 63 - np.abs(newv[i] - 63)\n",
        "            vel[i] = -vel[i]\n",
        "        elif newv[i] < 0:\n",
        "            newv[i] = np.abs(newv[i])\n",
        "            vel[i] = -vel[i]\n",
        "    return newv\n",
        "\n",
        "\n",
        "particle = dt.Sequential(particle, position=get_position)\n",
        "\n",
        "# Defining properties of the microscope\n",
        "optics = dt.Fluorescence(\n",
        "    NA=1,\n",
        "    output_region=(0, 0, IMAGE_SIZE, IMAGE_SIZE),\n",
        "    magnification=10,\n",
        "    resolution=(1e-6, 1e-6, 1e-6),\n",
        "    wavelength=633e-9,\n",
        ")\n",
        "\n",
        "# Combining everything into a dataset.\n",
        "# Note that the sequences are flipped in different directions, so that each unique sequence defines\n",
        "# in fact 8 sequences flipped in different directions, to speed up data generation\n",
        "sequential_images = dt.Sequence(\n",
        "    optics(particle ** (lambda: 1 + np.random.randint(MAX_PARTICLES))),\n",
        "    sequence_length=seq_len,\n",
        ")\n",
        "dataset_b2 = sequential_images >> dt.FlipUD() >> dt.FlipDiagonal() >> dt.FlipLR()"
      ],
      "metadata": {
        "id": "Q9jyhWbbfE6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_out_list = []\n",
        "encoder_label_list = []\n",
        "\n",
        "for seq in dataset_b2:\n",
        "  true_seq = tf.convert_to_tensor(seq).numpy()\n",
        "  for i in range(true_seq.shape[0]):\n",
        "    true_seq[i] = true_seq[i] / np.max(true_seq[i])\n",
        "  break\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(true_seq[9])"
      ],
      "metadata": {
        "id": "WQI8tSD4fQdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_encoded = encoder.predict(true_seq[:9], verbose=0)\n",
        "pred_list = []\n",
        "true_list = []\n",
        "\n",
        "for i in range(seq_len):\n",
        "  seq_encoded = tf.expand_dims(seq_encoded, 0)\n",
        "\n",
        "  # pred_encoded = transformer.predict(seq_encoded, verbose=0) # Transformer\n",
        "  pred_encoded = lstm.predict(seq_encoded, verbose=0) # LSTM\n",
        "\n",
        "  pred_decoded = decoder.predict(pred_encoded, verbose=0)\n",
        "  pred_decoded = pred_decoded / np.max(pred_decoded)\n",
        "  pred_list.append(pred_decoded[0])\n",
        "  pred_encoded_next = encoder.predict(pred_decoded, verbose=0)\n",
        "  seq_encoded = np.vstack([seq_encoded[0], pred_encoded_next[0]])\n",
        "  seq_encoded = np.delete(seq_encoded, (0), axis=0)"
      ],
      "metadata": {
        "id": "56Xn7tj_I5yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = 0\n",
        "plt.figure()\n",
        "plt.imshow(true_seq[9+y])\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(pred_list[y])"
      ],
      "metadata": {
        "id": "RhOUl1LdKWec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoders = 3\n",
        "num_att_gates = 3\n",
        "num_lstms = 4\n",
        "version = 6\n",
        "num_frames = seq_len - 10 - 40\n",
        "\n",
        "# location = \"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/3b/encoders={}_gates={}_v{}/\".format(num_encoders, num_att_gates, version) # Transformer\n",
        "location = \"/content/drive/MyDrive/Skola/CAS/Advanced Machine Learning/HW_B/3c/lstms={}_v{}/\".format(num_lstms, version) # LSTM\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5))\n",
        "\n",
        "for y in range(num_frames):\n",
        "  ax1.imshow(pred_list[y])\n",
        "  ax1.set_title('Pred.')\n",
        "\n",
        "  ax2.imshow(true_seq[9+y])\n",
        "  ax2.set_title('True')\n",
        "\n",
        "  fig.suptitle('Frame {}'.format(y+1))\n",
        "\n",
        "  title = 'frame={}'.format(y+1)\n",
        "  plt.savefig(location+title)"
      ],
      "metadata": {
        "id": "luhpCPIrAjNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# create a list of all image files in the directory\n",
        "image_files = [location + \"/\" + file for file in os.listdir(location) if file.endswith(\".png\")]\n",
        "\n",
        "# create a list of image objects from the image files\n",
        "images = [Image.open(file) for file in image_files]\n",
        "\n",
        "# create a GIF from the image sequence\n",
        "images[0].save(location+'sim.gif', save_all=True, append_images=images[1:], optimize=False, duration=200, loop=0)"
      ],
      "metadata": {
        "id": "d8ENpX5E6C8x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv_chalmers",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuClass": "premium"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}